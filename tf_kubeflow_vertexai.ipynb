{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78a6bcd9-92d5-4c77-bf3f-719a9f304f66",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h1>End-to-end TF KubeFlow pipleline on Vertex AI</h1>\n",
       "\n",
       "<img width=800 src='images/architecture.png'/>\n",
       "\n",
       "This is the <a href='https://github.com/GoogleCloudPlatform/vertex-pipelines-end-to-end-samples'>example</a> of architecture we are building overall. In this notebook, we are creating and running the KubeFlow pipeline on Vertex AI where data preparation to training is done in TensorFlow .\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<h1>End-to-end TF KubeFlow pipleline on Vertex AI</h1>\n",
    "\n",
    "<img width=800 src='images/architecture.png'/>\n",
    "\n",
    "This is the <a href='https://github.com/GoogleCloudPlatform/vertex-pipelines-end-to-end-samples'>example</a> of architecture we are building overall. In this notebook, we are creating and running the KubeFlow pipeline on Vertex AI where data preparation to training is done in TensorFlow ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ffcebfd-e7e6-4ec8-8b7c-efc33de14913",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h2>Useful links</h2>\n",
       "\n",
       "This is going to be based on example mentioned in the links:\n",
       "<br/>\n",
       "<a href=\"https://cloud.google.com/vertex-ai/docs/pipelines/build-pipeline\">It shows how to submit the KubeFlow pipeline to Vertex AI</a>\n",
       "<br/>\n",
       "<a href=\"https://medium.com/@lorenzo.colombi/kubeflow-pipeline-v2-tutorial-end-to-end-mnist-classifier-example-dc66714c2649\"> KubeFlow pipeline code example </a>\n",
       "<br/>\n",
       "<a href=\"https://drive.google.com/file/d/1vykV30ic2CrJxTnD9Cie1ISEfTgczPKm/view?usp=drive_link\">Shows how to compile</a>\n",
       "<br/>\n",
       "<a href=\"https://medium.com/@outsidenoxvodafone/how-to-use-google-cloud-vertex-ai-to-build-a-ml-pipeline-using-kubeflow-de61efa26fb3\">Good example</a> \n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<h2>Useful links</h2>\n",
    "\n",
    "This is going to be based on example mentioned in the links:\n",
    "<br/>\n",
    "<a href=\"https://cloud.google.com/vertex-ai/docs/pipelines/build-pipeline\">It shows how to submit the KubeFlow pipeline to Vertex AI</a>\n",
    "<br/>\n",
    "<a href=\"https://medium.com/@lorenzo.colombi/kubeflow-pipeline-v2-tutorial-end-to-end-mnist-classifier-example-dc66714c2649\"> KubeFlow pipeline code example </a>\n",
    "<br/>\n",
    "<a href=\"https://drive.google.com/file/d/1vykV30ic2CrJxTnD9Cie1ISEfTgczPKm/view?usp=drive_link\">Shows how to compile</a>\n",
    "<br/>\n",
    "<a href=\"https://medium.com/@outsidenoxvodafone/how-to-use-google-cloud-vertex-ai-to-build-a-ml-pipeline-using-kubeflow-de61efa26fb3\">Good example</a> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a812ad0-a4e7-49a3-8ec5-4480df59b660",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "pip install kfp>2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfcf4df5-af9a-45d2-8ede-c2872f71a877",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import kfp\n",
    "from kfp import dsl\n",
    "from kfp.dsl import Input, Output, Dataset, Model, Metrics, ClassificationMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "38131f0a-b2f4-429a-8233-e9a06b5b2bc0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<h3> Project Config</h3>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "\n",
    "<h3> Project Config</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d421cee1-89d1-4cd7-b943-8600b5ef995b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PIPELINE_ROOT: gs://mlops-heavy-workflow-bucket/pipeline_root/mnist-vertex-pipelines\n"
     ]
    }
   ],
   "source": [
    "PIPELINE_NAME = 'mnist-vertex-pipelines'\n",
    "GCS_BUCKET_NAME = 'mlops-heavy-workflow-bucket'\n",
    "\n",
    "PROJECT_ID = 'mlops-heavy-workflow-project'\n",
    "LOCATION = 'us-central1'\n",
    "JOBID = f\"training-pipeline-1\"\n",
    "ENABLE_CACHING = False\n",
    "\n",
    "# Path to various pipeline artifact.\n",
    "PIPELINE_ROOT = 'gs://{}/pipeline_root/{}'.format(\n",
    "    GCS_BUCKET_NAME, PIPELINE_NAME)\n",
    "\n",
    "print('PIPELINE_ROOT: {}'.format(PIPELINE_ROOT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e995d30c-c23a-426d-9301-e93c0e4b3bac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h3>Load Dataset</h3>\n",
       "In this step, we simply load the MNIST dataset from Keras. In this example, we use Kubeflow input/output artifacts. Artifacts are used to pass data between pipeline steps and are inspectable in the DAG (Directed Acyclic Graph) UI pipeline representation. Both input and output artifacts have to be declared as function parameters.\n",
       "<br/>\n",
       "\n",
       "For this step, we set Tensorflow as the base container image. Let’s remember how every step (Python function) will be encapsulated in a container. So, we need to choose an image that has all the necessary modules installed, or specify which modules are needed.\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<h3>Load Dataset</h3>\n",
    "In this step, we simply load the MNIST dataset from Keras. In this example, we use Kubeflow input/output artifacts. Artifacts are used to pass data between pipeline steps and are inspectable in the DAG (Directed Acyclic Graph) UI pipeline representation. Both input and output artifacts have to be declared as function parameters.\n",
    "<br/>\n",
    "\n",
    "For this step, we set Tensorflow as the base container image. Let’s remember how every step (Python function) will be encapsulated in a container. So, we need to choose an image that has all the necessary modules installed, or specify which modules are needed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dca20a60-e0be-400f-8377-5f32a778e4c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.component(base_image=\"tensorflow/tensorflow\", packages_to_install=['google-cloud-storage'])\n",
    "def load_dataset(x_train_artifact: Output[Dataset], x_test_artifact: Output[Dataset],y_train_artifact: Output[Dataset],y_test_artifact: Output[Dataset], bucket_name:str):\n",
    "    \n",
    "    import numpy as np\n",
    "    from tensorflow import keras\n",
    "    import os\n",
    "    from tensorflow.python.lib.io import file_io\n",
    "    import pickle\n",
    "    from google.cloud import storage\n",
    "   \n",
    "    (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "    \n",
    "    # we needed to use FILEIO as the open('file_name') was throwing some errors\n",
    "    np.save(file_io.FileIO(x_train_artifact.uri, 'w'),x_train)\n",
    "    np.save(file_io.FileIO(x_test_artifact.uri, 'w'),x_test)\n",
    "    np.save(file_io.FileIO(y_train_artifact.uri, 'w'),y_train)\n",
    "    np.save(file_io.FileIO(y_test_artifact.uri, 'w'),y_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "04ee8a21-cc69-4e07-b2c9-bb5fcac6c006",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h3> Preprocessing / Feature engineering</h3>\n",
       "This step takes as input the Dataset object created by the previous step. It’s essentially a simplified mock version of a preprocessing stage, performing actions like reshaping and resizing the arrays. I chose to include this step to make the pipeline more realistic. Afterwards, the dataset is saved using an output Artifact, once again.\n",
       "<br/>\n",
       "Here, we use for the first time Metrics object as output. This is a simple artifact object for storing key-value scalar metrics. I\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<h3> Preprocessing / Feature engineering</h3>\n",
    "This step takes as input the Dataset object created by the previous step. It’s essentially a simplified mock version of a preprocessing stage, performing actions like reshaping and resizing the arrays. I chose to include this step to make the pipeline more realistic. Afterwards, the dataset is saved using an output Artifact, once again.\n",
    "<br/>\n",
    "Here, we use for the first time Metrics object as output. This is a simple artifact object for storing key-value scalar metrics. I\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72a015b4-ef72-48ac-8c0b-f2a18523ab0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.component(base_image=\"tensorflow/tensorflow\")\n",
    "def preprocessing(metrics : Output[Metrics], x_train_processed : Output[Dataset], x_test_processed: Output[Dataset],\n",
    "                  x_train_artifact: Input[Dataset], x_test_artifact: Input[Dataset]):\n",
    "    ''' \n",
    "    just reshape and normalize data\n",
    "    '''\n",
    "    import numpy as np\n",
    "    import os\n",
    "    from io import BytesIO\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.python.lib.io import file_io\n",
    "    \n",
    "    # load data artifact store\n",
    "\n",
    "    x_train = np.load(BytesIO(file_io.read_file_to_string(x_train_artifact.uri, binary_mode=True)))\n",
    "    x_test = np.load(BytesIO(file_io.read_file_to_string(x_test_artifact.uri, binary_mode=True)))\n",
    "\n",
    "    # reshaping the data\n",
    "    # reshaping pixels in a 28x28px image with greyscale, canal = 1. This is needed for the Keras API\n",
    "    x_train = x_train.reshape(-1,28,28,1)\n",
    "    x_test = x_test.reshape(-1,28,28,1)\n",
    "    # normalizing the data\n",
    "    # each pixel has a value between 0-255. Here we divide by 255, to get values from 0-1\n",
    "    x_train = x_train / 255\n",
    "    x_test = x_test / 255\n",
    "    \n",
    "    #logging metrics using Kubeflow Artifacts\n",
    "    metrics.log_metric(\"Len x_train\", x_train.shape[0])\n",
    "    metrics.log_metric(\"Len y_train\", x_test.shape[0])\n",
    "   \n",
    "    \n",
    "    # save feuture in artifact store\n",
    "    np.save(file_io.FileIO(x_train_processed.uri, 'w'),x_train)\n",
    "    np.save(file_io.FileIO(x_test_processed.uri, 'w'),x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b598c57b-2442-43eb-8c51-b0461d1df586",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "Artifacts serve a dual purpose: they facilitate the transfer of data between steps and provide a means to display data within the UI. Moreover, all the artifacts are saved permanently in the metadata store.\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "\n",
    "Artifacts serve a dual purpose: they facilitate the transfer of data between steps and provide a means to display data within the UI. Moreover, all the artifacts are saved permanently in the metadata store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c30e57b3-f4cf-4fe6-8467-9142e892b50a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<h3> Model Definition</h3>\n",
       "\n",
       "The next step’s goal is to specify the neural network architecture. This way, if we will want to change the model structure in the future we will only need to change this component. Other components are indipendent and they use the model as defined here.\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "\n",
    "<h3> Model Definition</h3>\n",
    "\n",
    "The next step’s goal is to specify the neural network architecture. This way, if we will want to change the model structure in the future we will only need to change this component. Other components are indipendent and they use the model as defined here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "760152d8-12ee-4ef5-963c-46a48ad6af69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.component(base_image=\"tensorflow/tensorflow\")\n",
    "def model_building(ml_model : Output[Model]):\n",
    "    '''\n",
    "    Define the model architecture\n",
    "    This way it's more simple to change the model architecture and all the steps and indipendent\n",
    "    '''\n",
    "    from tensorflow import keras\n",
    "    import tensorflow as tf\n",
    "    import os\n",
    "    from pathlib import Path\n",
    "    \n",
    "    #model definition\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Conv2D(64, (3, 3), activation='relu', input_shape=(28,28,1)))\n",
    "    model.add(keras.layers.MaxPool2D(2, 2))\n",
    "\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dense(64, activation='relu'))\n",
    "    model.add(keras.layers.Dense(32, activation='relu'))\n",
    "\n",
    "    model.add(keras.layers.Dense(10, activation='softmax'))\n",
    "    \n",
    "    #saving model\n",
    "    model.save(ml_model.uri+\".keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "64e0863e-a808-4599-bbc3-b7e14fb84038",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<h3>Model Training</h3>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "\n",
    "<h3>Model Training</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07b3a3af-bd13-42bc-a46c-0afc39b9631f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.component(base_image=\"tensorflow/tensorflow\", packages_to_install=['scikit-learn'])\n",
    "def model_training(\n",
    "    ml_model : Input[Model],\n",
    "    x_train_processed : Input[Dataset], x_test_processed: Input[Dataset],\n",
    "    y_train_artifact : Input[Dataset], y_test_artifact :Input[Dataset],\n",
    "    metrics: Output[Metrics], classification_metrics: Output[ClassificationMetrics], model_trained: Output[Model]\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Build the model with Keras API\n",
    "    Export model metrics\n",
    "    \"\"\"\n",
    "    from tensorflow import keras\n",
    "    import tensorflow as tf\n",
    "    import numpy as np\n",
    "    import os\n",
    "    import glob\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    from io import BytesIO\n",
    "    from tensorflow.python.lib.io import file_io\n",
    "    from pathlib import Path\n",
    "    \n",
    "    #load dataset\n",
    "    \n",
    "    x_train = np.load(BytesIO(file_io.read_file_to_string(x_train_processed.uri, binary_mode=True)))\n",
    "    x_test = np.load(BytesIO(file_io.read_file_to_string(x_test_processed.uri, binary_mode=True)))\n",
    "    y_train = np.load(BytesIO(file_io.read_file_to_string(y_train_artifact.uri, binary_mode=True)))\n",
    "    y_test = np.load(BytesIO(file_io.read_file_to_string(y_test_artifact.uri, binary_mode=True)))\n",
    "    \n",
    "    #load model structure\n",
    "    model = keras.models.load_model(ml_model.uri+\".keras\")\n",
    "    \n",
    "    #reading best hyperparameters from katib\n",
    "    lr=0.001#float(hyperparameters[\"lr\"])\n",
    "    no_epochs = 1 #int(hyperparameters[\"num_epochs\"])\n",
    "    \n",
    "    #compile the model - we want to have a binary outcome\n",
    "    model.compile(tf.keras.optimizers.SGD(learning_rate=lr),\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    \n",
    "    #fit the model and return the history while training\n",
    "    history = model.fit(\n",
    "      x=x_train,\n",
    "      y=y_train,\n",
    "      epochs=no_epochs,\n",
    "      batch_size=20,\n",
    "    )\n",
    "\n",
    "     \n",
    "    # Test the model against the test dataset\n",
    "    # Returns the loss value & metrics values for the model in test mode.\n",
    "    model_loss, model_accuracy = model.evaluate(x=x_test,y=y_test)\n",
    "    \n",
    "    #build a confusione matrix\n",
    "    y_predict = model.predict(x=x_test)\n",
    "    y_predict = np.argmax(y_predict, axis=1)\n",
    "    cmatrix = confusion_matrix(y_test, y_predict)\n",
    "    cmatrix = cmatrix.tolist()\n",
    "    numbers_list = ['0','1','2','3','4','5','6','7','8','9']\n",
    "    #log confusione matrix\n",
    "    classification_metrics.log_confusion_matrix(numbers_list,cmatrix)\n",
    "  \n",
    "    #Kubeflox metrics export\n",
    "    metrics.log_metric(\"Test loss\", model_loss)\n",
    "    metrics.log_metric(\"Test accuracy\", model_accuracy)\n",
    "    \n",
    "    #keras.models.save_model(model,model_trained.uri + '.keras')\n",
    "    \n",
    "    # This will save the model with saved_mode.pb format which is required for deploying to Vertext AI.\n",
    "    # https://cloud.google.com/vertex-ai/docs/model-registry/import-model#aiplatform_upload_model_sample-python_vertex_ai_sdk\n",
    "    tf.saved_model.save(model, model_trained.uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "093a83ce-3bbb-48d0-82ef-2d175af5827e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<h3>Deploying the trained model to model registry and Entrypoint</h3>\n",
       "\n",
       "<a href=\"https://medium.com/@wardarahim25/step-by-step-guide-to-creating-and-deploying-custom-ml-pipelines-with-gcp-vertex-ai-part-2-3be6e314bc48#1592\">Good example</a>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "\n",
    "<h3>Deploying the trained model to model registry and Entrypoint</h3>\n",
    "\n",
    "<a href=\"https://medium.com/@wardarahim25/step-by-step-guide-to-creating-and-deploying-custom-ml-pipelines-with-gcp-vertex-ai-part-2-3be6e314bc48#1592\">Good example</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a581190a-47ac-42ea-adfa-97bf2d6d2468",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/kfp/dsl/component_decorator.py:119: FutureWarning: Python 3.7 has reached end-of-life. The default base_image used by the @dsl.component decorator will switch from 'python:3.7' to 'python:3.8' on April 23, 2024. To ensure your existing components work with versions of the KFP SDK released after that date, you should provide an explicit base_image argument and ensure your component works as intended on Python 3.8.\n",
      "  return component_factory.create_component_from_func(\n"
     ]
    }
   ],
   "source": [
    "@dsl.component(packages_to_install=['google-cloud-aiplatform'])\n",
    "def deploy_to_endpoint(\n",
    "    project: str,\n",
    "    display_name: str,\n",
    "    location: str,\n",
    "    model: Input[Model],\n",
    "    vertex_model: Output[Model],\n",
    "    vertex_endpoint: Output[Model]\n",
    "):\n",
    "    from google.cloud import aiplatform as vertex_ai\n",
    "    from pathlib import Path\n",
    "    \n",
    "    # Checks existing Vertex AI Enpoint or creates Endpoint if it is not exist.\n",
    "    def create_endpoint ():\n",
    "        endpoints = vertex_ai.Endpoint.list(\n",
    "        filter='display_name=\"{}\"'.format(display_name),\n",
    "        order_by='create_time desc',\n",
    "        project=project,\n",
    "        location=location,\n",
    "        )\n",
    "        if len(endpoints) > 0:\n",
    "            endpoint = endpoints[0] # most recently created\n",
    "        else:\n",
    "            endpoint = vertex_ai.Endpoint.create(\n",
    "                display_name=display_name,\n",
    "                project=project,\n",
    "                location=location\n",
    "        )\n",
    "        return endpoint\n",
    "\n",
    "    endpoint = create_endpoint()\n",
    "    \n",
    "    # Uploads trained model to Vertex AI Model Registry or creates new model version into existing uploaded one.\n",
    "    def upload_model ():\n",
    "        listed_model = vertex_ai.Model.list(\n",
    "        filter='display_name=\"{}\"'.format(display_name),\n",
    "        project=project,\n",
    "        location=location,\n",
    "        )\n",
    "        if len(listed_model) > 0:\n",
    "            model_version = listed_model[0] # most recently created\n",
    "            model_upload = vertex_ai.Model.upload(\n",
    "                    display_name=display_name,\n",
    "                    parent_model=model_version.resource_name,\n",
    "                    artifact_uri=model.uri,\n",
    "                    serving_container_image_uri='us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-11:latest',\n",
    "                    location=gcp_region,\n",
    "                    serving_container_predict_route=\"/predict\",\n",
    "                    serving_container_health_route=\"/health\"\n",
    "            )\n",
    "        else:\n",
    "            model_upload = vertex_ai.Model.upload(\n",
    "                    display_name=display_name,\n",
    "                    artifact_uri=model.uri,\n",
    "                    serving_container_image_uri='us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-11:latest',\n",
    "                    location=location,\n",
    "                    serving_container_predict_route=\"/predict\",\n",
    "                    serving_container_health_route=\"/health\"\n",
    "            )\n",
    "            \n",
    "        return model_upload\n",
    "    \n",
    "    uploaded_model = upload_model()\n",
    "    \n",
    "    # Save data to the output params\n",
    "    vertex_model.uri = uploaded_model.resource_name\n",
    "\n",
    "    # Deploys trained model to Vertex AI Endpoint\n",
    "    model_deploy = uploaded_model.deploy(\n",
    "        machine_type='n1-standard-4',\n",
    "        endpoint=endpoint,\n",
    "        traffic_split={\"0\": 100},\n",
    "        deployed_model_display_name=display_name,\n",
    "    )\n",
    "\n",
    "    # Save data to the output params\n",
    "    vertex_endpoint.uri = model_deploy.resource_name\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3db44c46-00aa-495c-a484-75961c193d54",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<h3>Pipeline Definition</h3>\n",
       "\n",
       "At the end we need to join all the pieces in a single pipeline. Note how the outputs of the various steps are used.\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "\n",
    "<h3>Pipeline Definition</h3>\n",
    "\n",
    "At the end we need to join all the pieces in a single pipeline. Note how the outputs of the various steps are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "46f644db-aea6-4753-86ac-b652811cc5cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TEMPLATE_PATH = \"mnist_pipeline.json\"\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name='mnist-classifier-dev',\n",
    "    description='Detect digits')\n",
    "def mnist_pipeline(bucket_name:str,project: str, display_name: str, location: str):\n",
    "    load_task = load_dataset(bucket_name=bucket_name)\n",
    "    load_task.set_caching_options(False)\n",
    "    preprocess_task = preprocessing(\n",
    "        x_train_artifact = load_task.outputs[\"x_train_artifact\"],\n",
    "        x_test_artifact = load_task.outputs[\"x_test_artifact\"]\n",
    "    )\n",
    "    preprocess_task.set_caching_options(False)\n",
    "    model_building_task = model_building()\n",
    "    model_building_task.set_caching_options(False)\n",
    "    training_task = model_training(\n",
    "        ml_model = model_building_task.outputs[\"ml_model\"],\n",
    "        x_train_processed = preprocess_task.outputs[\"x_train_processed\"],\n",
    "        x_test_processed = preprocess_task.outputs[\"x_test_processed\"],\n",
    "        y_train_artifact = load_task.outputs[\"y_train_artifact\"],\n",
    "        y_test_artifact = load_task.outputs[\"y_test_artifact\"]\n",
    "    )\n",
    "    training_task.set_caching_options(False)\n",
    "    deploy_to_endpoint(\n",
    "        model = training_task.outputs['model_trained'],\n",
    "        project = project,\n",
    "        location = location, \n",
    "        display_name = display_name\n",
    "    )\n",
    "    \n",
    "kfp.compiler.Compiler().compile(mnist_pipeline, package_path=TEMPLATE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce389c25-d1f9-4338-ad41-34abe1b52fc6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<h3>Submitting to Vertex AI</h3>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "\n",
    "<h3>Submitting to Vertex AI</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "77a61d7c-4277-4227-af9a-235b4fad0d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/605364818408/locations/us-central1/pipelineJobs/mnist-classifier-dev-20240529080737\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/605364818408/locations/us-central1/pipelineJobs/mnist-classifier-dev-20240529080737')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/mnist-classifier-dev-20240529080737?project=605364818408\n"
     ]
    }
   ],
   "source": [
    "import google.cloud.aiplatform as aip\n",
    "\n",
    "aip.init(\n",
    "    project=PROJECT_ID,\n",
    "    location=LOCATION,\n",
    ")\n",
    "\n",
    "PIPELINE_PARAMS = {\n",
    "    \"bucket_name\":GCS_BUCKET_NAME,\n",
    "    \"display_name\":PIPELINE_NAME,\n",
    "    \"project\":PROJECT_ID,\n",
    "    \"location\":LOCATION\n",
    "    \n",
    "}\n",
    "\n",
    "# Prepare the pipeline job\n",
    "job = aip.PipelineJob(\n",
    "    enable_caching=False,\n",
    "    display_name=PIPELINE_NAME,\n",
    "    template_path=TEMPLATE_PATH,\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    parameter_values=PIPELINE_PARAMS\n",
    ")\n",
    "\n",
    "job.submit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e70cbc1-17a7-4003-8859-c0177c26f669",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b80dd39-6f50-4257-84ad-c9e0ff65ccd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-11.m121",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-cpu.2-11:m121"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
